# Sentence Representation, Natural Language Inference and Summarization

> Including **Sentence Embeddings/Representations**, **Natural Language Inference**, **Sentence Matching**, **Textual Entailment** and etc.

## Sentence Embedding / Representation
- [2015 NIPS] **Skip Thought Vectors**, [[paper]](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf), [[bibtex]](/Bibtex/Skip%20Thought%20Vectors.bib), sources: [[ryankiros/skip-thoughts]](https://github.com/ryankiros/skip-thoughts).
- [2017 ICLR] **A Simple But Tough-to-beat Baseline for Sentence Embeddings**, [[paper]](https://openreview.net/pdf?id=SyK00v5xx), [[bibtex]](/Bibtex/A%20Simple%20But%20Tough-to-beat%20Baseline%20for%20Sentence%20Embeddings.bib), sources: [[PrincetonML/SIF]](https://github.com/PrincetonML/SIF).
- [2017 ICLR] **A Structured Self-attentive Sentence Embedding**, [[paper]](https://arxiv.org/pdf/1703.03130.pdf), [[bibtex]](/Bibtex/A%20Structured%20Self-attentive%20Sentence%20Embedding.bib), sources: [[ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding]](https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding), [[flrngel/Self-Attentive-tensorflow]](https://github.com/flrngel/Self-Attentive-tensorflow), [[kaushalshetty/Structured-Self-Attention]](https://github.com/kaushalshetty/Structured-Self-Attention).
- [2017 EMNLP] **Supervised Learning of Universal Sentence Representations from Natural Language Inference Data**, [[paper]](http://aclweb.org/anthology/D17-1070), [[bibtex]](/Bibtex/Supervised%20Learning%20of%20Universal%20Sentence%20Representations%20from%20Natural%20Language%20Inference%20Data.bib), sources: [[facebookresearch/InferSent]](https://github.com/facebookresearch/InferSent).
- [2018 ICLR] **Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning**, [[paper]](https://openreview.net/pdf?id=B18WgG-CZ), [[bibtex]](/Bibtex/Learning%20General%20Purpose%20Distributed%20Sentence%20Representations%20via%20Large%20Scale%20Multi-task%20Learning.bib), sources: [[Maluuba/gensen]](https://github.com/Maluuba/gensen).
- [2018 ArXiv] **Universal Sentence Encoder**, [[paper]](https://arxiv.org/pdf/1803.11175.pdf), [[bibtex]](/Bibtex/Universal%20Sentence%20Encoder.bib), sources: [[TensorFlow Hub/universal-sentence-encoder]](https://tfhub.dev/google/universal-sentence-encoder/1), [[helloeve/universal-sentence-encoder-fine-tune]](https://github.com/helloeve/universal-sentence-encoder-fine-tune).
- [2018 ArXiv] **Evaluation of Sentence Embeddings in Downstream and Linguistic Probing Tasks**, [[paper]](https://arxiv.org/pdf/1806.06259.pdf), [[bibtex]](/Bibtex/Evaluation%20of%20sentence%20embeddings%20in%20downstream%20and%20linguistic%20probing%20tasks.bib).
- [2018 EMNLP] **XNLI: Evaluating Cross-lingual Sentence Representations**, [[paper]](http://aclweb.org/anthology/D18-1269), [[bibtex]](/Bibtex/XNLI%20-%20Evaluating%20Cross-lingual%20Sentence%20Representations.bib), sources: [[facebookresearch/XNLI]](https://github.com/facebookresearch/XNLI).
- [2018 EMNLP] **Dynamic Meta-Embeddings for Improved Sentence Representations**, [[paper]](http://aclweb.org/anthology/D18-1176), [[bibtex]](/Bibtex/Dynamic%20Meta-Embeddings%20for%20Improved%20Sentence%20Representations.bib), sources: [[facebookresearch/DME]](https://github.com/facebookresearch/DME).
- [2018 ICLR] **An Efficient Framework for Learning Sentence Representations**, [[paper]](https://openreview.net/pdf?id=rJvJXZb0W), [[bibtex]](/Bibtex/An%20Efficient%20Framework%20for%20Learning%20Sentence%20Representations.bib), sources: [[lajanugen/S2V]](https://github.com/lajanugen/S2V), [[mhiro2/quick-thoughts]](https://github.com/mhiro2/quick-thoughts).
- [2018 ACL] **Sentence-State LSTM for Text Representation**, [[paper]](https://www.aclweb.org/anthology/P18-1030.pdf), [[bibtex]](/Bibtex/Sentence-State%20LSTM%20for%20Text%20Representation.bib), [[poster]](https://www.aclweb.org/anthology/attachments/P18-1030.Poster.pdf), sources: [[leuchine/S-LSTM]](https://github.com/leuchine/S-LSTM), [[bill-kalog/S-LSTM_pytorch]](https://github.com/bill-kalog/S-LSTM_pytorch).

## Natural Language Inference (Textual Entailment, Sentence Matching)
- [2016 NAACL] **Learning Natural Language Inference with LSTM**, [[paper]](http://www.aclweb.org/anthology/N16-1170), [[bibtex]](/Bibtex/Learning%20Natural%20Language%20Inference%20with%20LSTM.bib), source: [[shuohangwang/SeqMatchSeq]](https://github.com/shuohangwang/SeqMatchSeq).
- [2017 IJCAI] **BiMPM: Bilateral Multi-Perspective Matching for Natural Language Sentences**, [[paper]](https://arxiv.org/pdf/1702.03814.pdf), [[bibtex]](/Bibtex/Bilateral%20Multi-Perspective%20Matching%20for%20Natural%20Language%20Sentences.bib), sources: [[zhiguowang/BiMPM]](https://github.com/zhiguowang/BiMPM).
- [2017 ArXiv] **Distance-based Self-Attention Network for Natural Language Inference**, [[paper]](https://arxiv.org/pdf/1712.02047.pdf), [[bibtex]](/Bibtex/Distance-based%20Self-Attention%20Network%20for%20Natural%20Language%20Inference.bib).
- [2017 ACL] **Enhanced LSTM for Natural Language Inference**, [[paper]](http://aclweb.org/anthology/P17-1152), [[bibtex]](/Bibtex/Enhanced%20LSTM%20for%20Natural%20Language%20Inference.bib), sources: [[lukecq1231/nli]](https://github.com/lukecq1231/nli), [[coetaur0/ESIM]](https://github.com/coetaur0/ESIM), [[HsiaoYetGun/ESIM]](https://github.com/HsiaoYetGun/ESIM), [[sdnr1/EBIM-NLI]](https://github.com/sdnr1/EBIM-NLI), [[JasonForJoy/ESIM-NLI]](https://github.com/JasonForJoy/ESIM-NLI).
- [2018 AAAI] **DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding**, [[paper]](https://arxiv.org/pdf/1709.04696.pdf), [[bibtex]](/Bibtex/DiSAN%20-%20Directional%20Self-Attention%20Network%20for%20RNN%20-%20CNN-free%20Language%20Understanding.bib), sources: [[taoshen58/DiSAN]](https://github.com/taoshen58/DiSAN).
- [2018 IJCAI] **Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling**, [[paper]](https://www.ijcai.org/proceedings/2018/0604.pdf), [[bibtex]](/Bibtex/Reinforced%20Self-Attention%20Network%20-%20a%20Hybrid%20of%20Hard%20and%20Soft%20Attention%20for%20Sequence%20Modeling.bib).
- [2018 IJCAI] **Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains**, [[paper]](https://www.ijcai.org/proceedings/2018/0615.pdf), [[bibtex]](/Bibtex/Hermitian%20Co-Attention%20Networks%20for%20Text%20Matching%20in%20Asymmetrical%20Domains.bib).
- [2019 ACL] **Multi-Task Deep Neural Networks for Natural Language Understanding**, [[paper]](https://www.aclweb.org/anthology/P19-1441.pdf), [[bibtex]](/Bibtex/Multi-Task%20Deep%20Neural%20Networks%20for%20Natural%20Language%20Understanding.bib), sources: [[namisan/mt-dnn]](https://github.com/namisan/mt-dnn).

## Sentence Representation Evaluation
- [2018 EMNLP] **GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding**, [[paper]](http://aclweb.org/anthology/W18-5446), [[bibtex]](/Bibtex/GLUE%20-%02A%20Multi-Task%20Benchmark%20and%20Analysis%20Platform%20for%20Natural%20Language%20Understanding.bib), [[homepage]](https://gluebenchmark.com), sources: [[nyu-mll/GLUE-baselines]](https://github.com/nyu-mll/GLUE-baselines).
- [2019 ICLR] **GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding**, [[paper]](https://openreview.net/pdf?id=rJ4km2R5t7), [[bibtex]](/Bibtex/GLUE%20-%02A%20Multi-Task%20Benchmark%20and%20Analysis%20Platform%20for%20Natural%20Language%20Understanding.bib), [[homepage]](https://gluebenchmark.com), sources: [[nyu-mll/GLUE-baselines]](https://github.com/nyu-mll/GLUE-baselines).
