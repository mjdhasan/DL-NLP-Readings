# Machine Translation and Language Style Transfer

## Machine Translation
- [2014 SSST] **On the properties of neural machine Translation Encoder-Decoder Approaches**, [[paper]](https://arxiv.org/pdf/1409.1259.pdf).
- [2015 ICLR] **Neural Machine Translation by Jointly Learning to Align and Translate**, [[paper]](https://arxiv.org/pdf/1409.0473.pdf), sources: [[lisa-groundhog/GroundHog]](https://github.com/lisa-groundhog/GroundHog/tree/master/experiments/nmt), [[tensorflow/nmt]](https://github.com/tensorflow/nmt).
- [2015 EMNLP] **Effective Approaches to Attention-based Neural Machine Translation**, [[paper]](http://aclweb.org/anthology/D15-1166), [[HarvardNLP homepage]](http://nlp.seas.harvard.edu/code/), sources: [[dillonalaird/Attention]](https://github.com/dillonalaird/Attention), [[tensorflow/nmt]](https://github.com/tensorflow/nmt).
- [2016 ACL] **Neural Machine Translation of Rare Words with Subword Units**, [[paper]](http://www.aclweb.org/anthology/P16-1162), [[bibtex]](/Bibtex/Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units.bib), [[software]](http://anthology.aclweb.org/attachments/P/P16/P16-1162.Software.zip), sources: [[rsennrich/subword-nmt]](https://github.com/rsennrich/subword-nmt), [[soaxelbrooke/python-bpe]](https://github.com/soaxelbrooke/python-bpe).
- [2016 NeurIPS] **Professor Forcing: A New Algorithm for Training Recurrent Networks**, [[paper]](http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf), [[bibtex]](https://scholar.googleusercontent.com/scholar.bib?q=info:Ora5k19Xru4J:scholar.google.com/&output=citation&scisdr=CgU1_ws_EOaSuk_nxaw:AAGBfm0AAAAAXrri3axipUR5fljkldvo72BcmW5PJ0XS&scisig=AAGBfm0AAAAAXrri3dRw1VDMLCcwfdPk2mr6RPJ2YRy0&scisf=4&ct=citation&cd=-1&hl=en), sources: [[anirudh9119/LM_GANS]](https://github.com/anirudh9119/LM_GANS).
- [2017 ACL] **A Convolutional Encoder Model for Neural Machine Translation**, [[paper]](https://arxiv.org/pdf/1611.02344.pdf), sources: [[facebookresearch/fairseq]](https://github.com/facebookresearch/fairseq).
- [2017 NIPS] **Attention is All You Need**, [[paper]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), [[Chinses blog]](http://www.cnblogs.com/robert-dlut/p/8638283.html), sources: [[Kyubyong/transformer]](https://github.com/Kyubyong/transformer), [[jadore801120/attention-is-all-you-need-pytorch]](https://github.com/jadore801120/attention-is-all-you-need-pytorch), [[DongjunLee/transformer-tensorflow]](https://github.com/DongjunLee/transformer-tensorflow).
- [2017 ICML] **Convolutional Sequence to Sequence Learning**, [[paper]](https://arxiv.org/pdf/1705.03122v3.pdf), [[bibtex]](/Bibtex/Convolutional%20Sequence%20to%20Sequence%20Learning.bib), sources: [[pytorch/fairseq]](https://github.com/pytorch/fairseq).
- [2017 EMNLP] **Neural Machine Translation with Word Predictions**, [[paper]](http://www.aclweb.org/anthology/D17-1013).
- [2017 EMNLP] **Massive Exploration of Neural Machine Translation Architectures**, [[paper]](http://aclweb.org/anthology/D17-1151), [[homepage]](https://google.github.io/seq2seq/), sources: [[google/seq2seq]](https://github.com/google/seq2seq).
- [2017 EMNLP] **Efficient Attention using a Fixed-Size Memory Representation**, [[paper]](http://aclweb.org/anthology/D17-1040).
- [2018 AMTA] **Context Models for OOV Word Translation in Low-Resource Language**, [[paper]](https://arxiv.org/pdf/1801.08660.pdf).
- [2018 ACL] **Modeling Localness for Self-Attention Networks**, [[paper]](https://www.aclweb.org/anthology/D18-1475.pdf), [[bibtex]](/Bibtex/Modeling%20Localness%20for%20Self-Attention%20Networks.bib).
- [2018 NAACL] **Self-Attention with Relative Position Representations**, [[paper]](https://www.aclweb.org/anthology/N18-2074.pdf), [[bibtex]](/Bibtex/Self-Attention%20with%20Relative%20Position%20Representations.bib).
- [2018 COLING] **Double Path Networks for Sequence to Sequence Learning**, [[paper]](https://arxiv.org/pdf/1806.04856.pdf).
- [2018 EMNLP] **Meta-Learning for Low-Resource Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/D18-1398.pdf), [[bibtex]](/Bibtex/Meta-Learning%20for%20Low-Resource%20Neural%20Machine%20Translation.bib).
- [2019 NAACL] **Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N19-1120), [[bibtex]](/Bibtex/Extract%20and%20Edit%20-%20An%20Alternative%20to%20Back-Translation%20for%20Unsupervised%20Neural%20Machine%20Translation.bib), sources: [[jiaweiw/Extract-Edit-Unsupervised-NMT]](https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT).
- [2019 NAACL] **Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/N19-1209), [[bibtex]](/Bibtex/Overcoming%20Catastrophic%20Forgetting%20During%20Domain%20Adaptation%20of%20Neural%20Machine%20Translation.bib).
- [2019 ACL] **From Bilingual to Multilingual Neural Machine Translation by Incremental Training**, [[paper]](https://www.aclweb.org/anthology/P19-2033), [[bibtex]](/Bibtex/From%20Bilingual%20to%20Multilingual%20Neural%20Machine%20Translation%20by%20Incremental%20Training.bib).
- [2019 ACL] **Bridging the Gap between Training and Inference for Neural Machine Translation**, [[paper]](https://www.aclweb.org/anthology/P19-1426.pdf), [[bibtex]](/Bibtex/Bridging%20the%20Gap%20between%20Training%20and%20Inference%20for%20Neural%20Machine%20Translation.bib), [[论文解释1]](https://spring-quan.github.io/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/), [[论文解释2]](https://zhuanlan.zhihu.com/p/76227765), sources: [[ictnlp/OR-NMT]](https://github.com/ictnlp/OR-NMT).
	- Gumbel-Max Technique: [2014 NIPS] **A\* Sampling**, [[paper]](https://papers.nips.cc/paper/5449-a-sampling.pdf), [[bibtex]](/Bibtex/A-Sampling.bib), [[Gumbel-max trick]](https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/), [[Gumbel trick]](https://blog.csdn.net/a358463121/article/details/80820878), [[Gumbel-Max Trick]](https://www.ntu.edu.sg/home/lixiucheng/paper/gumbel-softmax.html), [[The Gumbel trick]](https://francisbach.com/the-gumbel-trick/), [[Gumbel Distribution]](https://blog.csdn.net/jackytintin/article/details/79364490).
- [2020 ICLR] **Neural Machine Translation with Universal Visual Representation**, [[paper]](https://openreview.net/pdf?id=Byl8hhNYPS), [[bibtex]](/Bibtex/Neural%20Machine%20Translation%20with%20Universal%20Visual%20Representation.bib), sources: [[cooelf/UVR-NMT]](https://github.com/cooelf/UVR-NMT).
- [2020 ArXiv] **Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation**, [[paper]](https://arxiv.org/pdf/2001.08140.pdf), [[bibtex]](https://scholar.googleusercontent.com/scholar.bib?q=info:n2ekHcvBI8AJ:scholar.google.com/&output=citation&scisdr=CgU1_ws_EMa_0lNn7So:AAGBfm0AAAAAXqZi9Sq117oTyjRZ9t5QVNNuEmiQbxPu&scisig=AAGBfm0AAAAAXqZi9VoRSYrzh3HX4bWW3pQDd3QQOVo7&scisf=4&ct=citation&cd=-1&hl=en).

## Language Style Transfer
- [2018 ArXiv] **Style Transfer as Unsupervised Machine Translation**, [[paper]](https://arxiv.org/pdf/1808.07894.pdf), [[bibtex]](/Bibtex/Style%20Transfer%20as%20Unsupervised%20Machine%20Translation.bib), [[homepage]](https://zrustc.github.io).
- [2019 ICLR] **Multiple-Attribute Text Rewriting**, [[paper]](https://openreview.net/pdf?id=H1g2NhC5KQ), [[bibtex]](/Bibtex/Multiple-Attribute%20Text%20Rewriting.bib).