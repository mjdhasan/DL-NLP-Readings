# Language Modeling Systems and Extensions

## Language Models
- sources: [[thunlp/PLMpapers]](https://github.com/thunlp/PLMpapers).
- sources: [[Jiakui/awesome-bert]](https://github.com/Jiakui/awesome-bert).
- **Transferring NLP models across languages and domains**, [[slides]](https://syntaxfest.github.io/syntaxfest19/slides/invited_talk_syntaxfest_plank.pdf).
- [2017 ICML] **Language Modeling with Gated Convolutional Networks**, [[paper]](https://arxiv.org/pdf/1612.08083.pdf), sources: [[anantzoid/Language-Modeling-GatedCNN]](https://github.com/anantzoid/Language-Modeling-GatedCNN), [[jojonki/Gated-Convolutional-Networks]](https://github.com/jojonki/Gated-Convolutional-Networks).
- [2017 NIPS] **Learned in Translation: Contextualized Word Vectors**, [[paper]](https://arxiv.org/pdf/1708.00107.pdf), sources: [[salesforce/cove]](https://github.com/salesforce/cove).
- [2018 ICLR] **Regularizing and Optimizing LSTM Language Models**, [[paper]](https://openreview.net/pdf?id=SyyGPP0TZ), [[bibtex]](/Bibtex/Regularizing%20and%20Optimizing%20LSTM%20Language%20Models.bib), sources: [[salesforce/awd-lstm-lm]](https://github.com/salesforce/awd-lstm-lm), author page: [[Nitish Shirish Keskar]](https://keskarnitish.github.io).
- [2018 NAACL] **Deep contextualized word representations**, [[paper]](https://arxiv.org/pdf/1802.05365.pdf), [[homepage]](https://allennlp.org/elmo), sources: [[allenai/bilm-tf]](https://github.com/allenai/bilm-tf), [[HIT-SCIR/ELMoForManyLangs]](https://github.com/HIT-SCIR/ELMoForManyLangs). Some extended application: [[UKPLab/elmo-bilstm-cnn-crf]](https://github.com/UKPLab/elmo-bilstm-cnn-crf).
- [2018 NIPS] **GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations**, [[paper]](https://arxiv.org/pdf/1806.05662.pdf), [[bibtex]](GLoMo%20-%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations.bib), sources: [[YJHMITWEB/GLoMo-tensorflow]](https://github.com/YJHMITWEB/GLoMo-tensorflow).
- [2018 ArXiv] **Improving Language Understanding by Generative Pre-Training**, [[paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [[bibtex]](/Bibtex/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.bib), [[homepage]](https://blog.openai.com/language-unsupervised/), sources: [[openai/finetune-transformer-lm]](https://github.com/openai/finetune-transformer-lm).
- [2019 AAAI] **Character-Level Language Modeling with Deeper Self-Attention**, [[paper]](https://arxiv.org/pdf/1808.04444.pdf), [[bibtex]](/Bibtex/Character-Level%20Language%20Modeling%20with%20Deeper%20Self-Attention.bib), sources: [[nadavbh12/Character-Level-Language-Modeling-with-Deeper-Self-Attention-pytorch]](https://github.com/nadavbh12/Character-Level-Language-Modeling-with-Deeper-Self-Attention-pytorch).
- [2019 NAACL] **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, [[paper]](https://www.aclweb.org/anthology/N19-1423.pdf), [[bibtex]](/Bibtex/BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.bib), [[slides]](https://nlp.stanford.edu/seminar/details/jdevlin.pdf), sources: [[google-research/bert]](https://github.com/google-research/bert), [[huggingface/pytorch-pretrained-BERT]](https://github.com/huggingface/pytorch-pretrained-BERT). Blog posts: 
  - daiwk的BERT解读: [I](https://daiwk.github.io/posts/nlp-bert.html), [II](https://daiwk.github.io/posts/nlp-bert-code-annotated-framework.html), [III](https://daiwk.github.io/posts/nlp-bert-code-annotated-application.html), [IV](https://daiwk.github.io/posts/nlp-bert-code.html)
  - Dissecting BERT: [I](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3), [II](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f), [III](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
  - The Illustrated Transformer: [EN](https://jalammar.github.io/illustrated-transformer/), [CN](https://zhuanlan.zhihu.com/p/54356280)
  - The Annotated Transformer: [EN](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
  - 从Word Embedding到BERT - NLP预训练技术发展史: [CN](https://zhuanlan.zhihu.com/p/49271699)
  - NLP三大特征抽取器(CNN/RNN/Transformer)比较: [CN](https://zhuanlan.zhihu.com/p/54743941)
- [2019 ACL] **Transformer-XL: Attentive Language Models beyond a Fixed-Length Context**, [[paper]](https://www.aclweb.org/anthology/P19-1285.pdf), [[bibtex]](/Bibtex/Transformer-XL%20-%20Attentive%20Language%20Models%20beyond%20a%20Fixed-Length%20Context.bib), [[post]](https://towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924), sources: [[kimiyoung/transformer-xl]](https://github.com/kimiyoung/transformer-xl).
- [2019 ICML] **BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning**, [[paper]](http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf), [[bibtex]](/Bibtex/BERT%20and%20PALs%20-%20Projected%20Attention%20Layers%20for%20Efficient%20Adaptation%20in%20Multi-Task%20Learning.bib), [[supplementary]](http://proceedings.mlr.press/v97/stickland19a/stickland19a-supp.pdf), sources: [[AsaCooperStickland/Bert-n-Pals]](https://github.com/AsaCooperStickland/Bert-n-Pals).
- [2019 ArXiv] **Cross-lingual Language Model Pretraining**, [[paper]](https://arxiv.org/pdf/1901.07291.pdf), [[bibtex]](/Bibtex/Cross-lingual%20Language%20Model%20Pretraining.bib), sources: [[facebookresearch/XLM]](https://github.com/facebookresearch/XLM).
- [2019 ArXiv] **GPT-2: Language Models are Unsupervised Multitask Learners**, [[paper]](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [[bibtex]](/Bibtex/Language%20Models%20are%20Unsupervised%20Multitask%20Learners.bib), [[homepage]](https://blog.openai.com/better-language-models/), sources: [[openai/gpt-2]](https://github.com/openai/gpt-2).
- [2019 ICLR] **What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations**, [[paper]](https://openreview.net/pdf?id=SJzSgnRcKX), [[bibtex]](/Bibtex/What%20Do%20You%20Learn%20from%20Context%20Probing%20for%20Sentence%20Structure%20in%20Contextualized%20Word%20Representations.bib).
- [2019 ICML] **MASS: Masked Sequence to Sequence Pre-training for Language Generation**, [[paper]](https://arxiv.org/pdf/1905.02450.pdf), [[bibtex]](/Bibtex/MASS%20-%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20Language%20Generation.bib), sources: [[xutaatmicrosoftdotcom/MASS]](https://github.com/xutaatmicrosoftdotcom/MASS).
- [2019 ACL] **ERNIE: Enhanced Language Representation with Informative Entities**, [[paper]](https://arxiv.org/pdf/1905.07129.pdf), [[bibtex]](/Bibtex/ERNIE%20-%20Enhanced%20Language%20Representation%20with%20Informative%20Entities.bib), [[blog]](https://www.jiqizhixin.com/articles/2019-05-26-4), sources: [[thunlp/ERNIE]](https://github.com/thunlp/ERNIE).
- [2019 IJCNLP] **Cloze-driven Pretraining of Self-attention Networks**, [[paper]](https://arxiv.org/pdf/1903.07785.pdf), [[bibtex]](/Bibtex/Cloze-driven%20Pretraining%20of%20Self-attention%20Networks.bib).
- [2019 NeurIPS] **XLNet: Generalized Autoregressive Pretraining for Language Understanding**, [[paper]](https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf), [[bibtex]](/Bibtex/XLNet%20-%20Generalized%20Autoregressive%20Pretraining%20for%20Language%20Understanding.bib), [[Supplementary]](https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding), sources: [[zihangdai/xlnet]](https://github.com/zihangdai/xlnet).
- [2019 ArXiv] **K-BERT: Enabling Language Representation with Knowledge Graph**, [[paper]](https://arxiv.org/pdf/1909.07606.pdf), [[bibtex]](/Bibtex/K-BERT%20-%20Enabling%20Language%20Representation%20with%20Knowledge%20Graph.bib).
- [2019 ArXiv] **M-BERT: Injecting Multimodal Information in the BERT Structure**, [[paper]](https://arxiv.org/pdf/1908.05787.pdf), [[bibtex]](/Bibtex/M-BERT%20-%20Injecting%20Multimodal%20Information%20in%20the%20BERT%20Structure.bib).
- [2019 ArXiv] **RoBERTa: A Robustly Optimized BERT Pretraining Approach**, [[paper]](https://arxiv.org/pdf/1907.11692.pdf), [[bibtex]](/Bibtex/RoBERTa%20-%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach.bib), sources: [[pytorch/fairseq]](https://github.com/pytorch/fairseq/tree/master/examples/roberta).
- [2019 ArXiv] **TinyBERT: Distilling BERT for Natural Language Understanding**, [[paper]](https://arxiv.org/pdf/1909.10351.pdf), [[bibtex]](/Bibtex/TinyBERT%20-%20Distilling%20BERT%20for%20Natural%20Language%20Understanding.bib).
- [2019 ArXiv] **Emerging Cross-lingual Structure in Pretrained Language Models**, [[paper]](https://arxiv.org/pdf/1911.01464.pdf), [[bibtex]](/Bibtex/Emerging%20Cross-lingual%20Structure%20in%20Pretrained%20Language%20Models.bib).
- [2019 ArXiv] **NeZha: Neural Contextualized Representation for Chinese Language Understanding**, [[paper]](https://arxiv.org/pdf/1909.00204.pdf), [[bibtex]](/Bibtex/NeZha%20-%20Neural%20Contextualized%20Representation%20for%20Chinese%20Language%20Understanding.bib), sources: [[huawei-noah/Pretrained-Language-Model/NEZHA]](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA).
- [2020 ICLR] **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**, [[paper]](https://openreview.net/pdf?id=r1xMH1BtvB), [[bibtex]](/Bibtex/ELECTRA%20-%20Pre-training%20Text%20Encoders%20as%20Discriminators%20Rather%20Than%20Generators.bib).
- [2020 ICLR] **ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations**, [[paper]](https://openreview.net/pdf?id=H1eA7AEtvS), [[bibtex]](/Bibtex/ALBERT%20-%20A%20Lite%20BERT%20for%20Self-Supervised%20Learning%20of%20Language%20Representations.bib), sources: [[google-research/ALBERT]](https://github.com/google-research/ALBERT).

## Language Model Analysis
- [2019 ArXiv] **How multilingual is Multilingual BERT?**, [[paper]](https://arxiv.org/pdf/1906.01502.pdf), [[bibtex]](/Bibtex/How%20multilingual%20is%20Multilingual%20BERT.bib).
- [2019 ACL] **What does BERT learn about the structure of language?**, [[paper]](https://www.aclweb.org/anthology/P19-1356.pdf), [[bibtex]](/Bibtex/What%20does%20BERT%20learn%20about%20the%20structure%20of%20language.bib), sources: [[ganeshjawahar/interpret_bert]](https://github.com/ganeshjawahar/interpret_bert).
- [2019 EMNLP] **Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT**, [[paper]](https://www.aclweb.org/anthology/D19-1077.pdf), [[bibtex]](/Bibtex/Beto%20Bentz%20Becas%20-%20The%20Surprising%20Cross-Lingual%20Effectiveness%20of%20BERT.bib), sources: [[shijie-wu/crosslingual-nlp]](https://github.com/shijie-wu/crosslingual-nlp).
- [2019 ArXiv] **What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?**, [[paper]](https://arxiv.org/pdf/1910.12391.pdf), [[bibtex]](/Bibtex/What%20does%20BERT%20Learn%20from%20Multiple-Choice%20Reading%20Comprehension%20Datasets.bib).