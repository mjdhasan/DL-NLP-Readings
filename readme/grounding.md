# Visual Grounding and Multi-Modal Learning

- Reading Resources: [[TheShadow29/awesome-grounding]](https://github.com/TheShadow29/awesome-grounding).
- Reading Resources: [[chingyaoc/awesome-vqa]](https://github.com/chingyaoc/awesome-vqa).

## Survey
- [2019 ArXiv] **Multimodal Intelligence: Representation Learning Information Fusion and Applications**, [[paper]](https://arxiv.org/pdf/1911.03977.pdf), [[bibtex]](/Bibtex/Multimodal%20Intelligence%20-%20Representation%20Learning%20Information%20Fusion%20and%20Applications.bib).

## Image-based Visual Grounding
### Vision-and-Language Modelling
- [2019 ArXiv] **M-BERT: Injecting Multimodal Information in the BERT Structure**, [[paper]](https://arxiv.org/pdf/1908.05787.pdf), [[bibtex]](/Bibtex/M-BERT%20-%20Injecting%20Multimodal%20Information%20in%20the%20BERT%20Structure.bib).
- [2019 NeurIPS] **ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks**, [[paper]](https://papers.nips.cc/paper/8297-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.pdf), [[bibtex]](/Bibtex/ViLBERT%20-%20Pretraining%20Task-Agnostic%20Visiolinguistic%20Representations%20for%20Vision-and-Language%20Tasks.bib), sources: [[jiasenlu/vilbert_beta]](https://github.com/jiasenlu/vilbert_beta).
- [2019 ArXiv] **VisualBERT: A Simple and Performant Baseline for Vision and Language**, [[paper]](https://arxiv.org/pdf/1908.03557.pdf), [[bibtex]](/Bibtex/VisualBERT%20-%20A%20Simple%20and%20Performant%20Baseline%20for%20Vision%20and%20Language.bib), sources: [[uclanlp/visualbert]](https://github.com/uclanlp/visualbert).
- [2019 EMNLP] **LXMERT: Learning Cross-Modality Encoder Representations from Transformers**, [[paper]](https://www.aclweb.org/anthology/D19-1514.pdf), [[bibtex]](/Bibtex/LXMERT%20-%20Learning%20Cross-Modality%20Encoder%20Representations%20from%20Transformers.bib), sources: [[airsplay/lxmert]](https://github.com/airsplay/lxmert).
- [2019 CVPR] **Multi-task Learning of Hierarchical Vision-Language Representation**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Nguyen_Multi-Task_Learning_of_Hierarchical_Vision-Language_Representation_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Multi-task%20Learning%20of%20Hierarchical%20Vision-Language%20Representation.bib).
- [2019 ArXiv] **UNITER: Learning Universal Image-Text Representations**, [[paper]](https://arxiv.org/pdf/1909.11740.pdf), [[bibtex]](/Bibtex/UNITER%20-%20Learning%20Universal%20Image-Text%20Representations.bib).
- [2020 AAAI] **Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training**, [[paper]](https://arxiv.org/pdf/1908.06066.pdf), [[bibtex]](/Bibtex/Unicoder-VL%20-%20A%20Universal%20Encoder%20for%20Vision%20and%20Language%20by%20Cross-modal%20Pre-training.bib).
- [2020 AAAI] **Unified Vision-Language Pre-Training for Image Captioning and VQA**, [[paper]](https://arxiv.org/pdf/1909.11059.pdf), [[bibtex]](/Bibtex/Unified%20Vision-Language%20Pre-Training%20for%20Image%20Captioning%20and%20VQA.bib), sources: [[LuoweiZhou/VLP]](https://github.com/LuoweiZhou/VLP).
- [2020 ICLR] **VL-BERT: Pre-training of Generic Visual-Linguistic Representations**, [[paper]](https://openreview.net/pdf?id=SygXPaEYvH), [[bibtex]](/Bibtex/VL-BERT%20-%20Pre-training%20of%20Generic%20Visual-Linguistic%20Representations.bib), sources: [[jackroos/VL-BERT]](https://github.com/jackroos/VL-BERT).
- [2020 ICLR] **Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling**, [[paper]](https://openreview.net/pdf?id=H1x5wRVtvS), [[bibtex]](/Bibtex/Variational%20Hetero-Encoder%20Randomized%20GANs%20for%20Joint%20Image-Text%20Modeling.bib).
- [2020 ArXiv] **Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers**, [[paper]](https://arxiv.org/pdf/2004.00849.pdf), [[bibtex]](/Bibtex/Pixel-BERT.bib).

### Image Retrieval
- [2019 CVPR] **Deep Supervised Cross-modal Retrieval**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhen_Deep_Supervised_Cross-Modal_Retrieval_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Deep%20Supervised%20Cross-modal%20Retrieval.bib).
- [2019 SIGIR] **Scalable Deep Multimodal Learning for Cross-Modal Retrieval**, [[paper]](/Documents/Papers/Scalable%20Deep%20Multimodal%20Learning%20for%20Cross-Modal%20Retrieval.pdf), [[bibtex]](/Bibtex/Scalable%20Deep%20Multimodal%20Learning%20for%20Cross-Modal%20Retrieval.bib).
- [2019 ACMMM] **A New Benchmark and Approach for Fine-grained Cross-media Retrieval**, [[paper]](https://arxiv.org/pdf/1907.04476.pdf), [[bibtex]](/Bibtex/A%20New%20Benchmark%20and%20Approach%20for%20Fine-grained%20Cross-media%20Retrieval.bib), [[homepage]](http://59.108.48.34/tiki/FGCrossNet/), sources: [[PKU-ICST-MIPL/FGCrossNet_ACMMM2019]](https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019).
- [2019 ICCV] **Adversarial Representation Learning for Text-to-Image Matching**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Adversarial%20Representation%20Learning%20for%20Text-to-Image%20Matching.bib).

### Image Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2015 NeurIPS] **Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks**, [[paper]](https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf), [[bibtex]](/Bibtex/Scheduled%20Sampling%20for%20Sequence%20Prediction%20with%20Recurrent%20Neural%20Networks.bib).
- [2016 NeurIPS] **Professor Forcing: A New Algorithm for Training Recurrent Networks**, [[paper]](http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf), [[bibtex]](/Bibtex/Professor%20Forcing.bib), sources: [[anirudh9119/LM_GANS]](https://github.com/anirudh9119/LM_GANS).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2017 CVPR] **SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/SCA-CNN%20-%20Spatial%20and%20Channel-wise%20Attention%20in%20Convolutional%20Networks%20for%20Image%20Captioning.bib), sources: [[zjuchenlong/sca-cnn.cvpr17]](https://github.com/zjuchenlong/sca-cnn.cvpr17).
- [2017 CVPR] **Self-critical Sequence Training for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Self-critical%20Sequence%20Training%20for%20Image%20Captioning.bib), sources: [[ruotianluo/self-critical.pytorch]](https://github.com/ruotianluo/self-critical.pytorch).
- [2018 CVPR] **Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Bottom-Up%20and%20Top-Down%20Attention%20for%20Image%20Captioning%20and%20Visual%20Question%20Answering.bib), sources: [[peteanderson80/bottom-up-attention]](https://github.com/peteanderson80/bottom-up-attention), [[hengyuan-hu/bottom-up-attention-vqa]](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [[LeeDoYup/bottom-up-attention-tf]](https://github.com/LeeDoYup/bottom-up-attention-tf).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).
- [2019 CVPR] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib), [[post]](https://zhuanlan.zhihu.com/p/41200392).
- [2019 ICCV] **Attention on Attention for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Attention%20on%20Attention%20for%20Image%20Captioning.bib), sources: [[husthuaan/AoANet]](https://github.com/husthuaan/AoANet).
- [2020 ArXiv] **Deconfounded Image Captioning: A Causal Retrospect**, [[paper]](https://arxiv.org/pdf/2003.03923.pdf), [[bibtex]](/Bibtex/Deconfounded%20image%20captioning.bib).

### Text-based Image Edit
- [2019 ACL] **Expressing Visual Relationships via Language**, [[paper]](https://www.aclweb.org/anthology/P19-1182.pdf), [[bibtex]](/Bibtex/Expressing%20Visual%20Relationships%20via%20Language.bib), sources: [[airsplay/VisualRelationships]](https://github.com/airsplay/VisualRelationships).

### Visual Question Answering and Visual Reasoning
- [2015 ICCV] **VQA: Visual Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/VQA%20-%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/index.html).
- [2016 NIPS] **Hierarchical Question-Image Co-Attention for Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1606.00061), [[bibtex]](/Bibtex/Hierarchical%20Question-Image%20Co-Attention%20for%20Visual%20Question%20Answering.bib), sources: [[karunraju/VQA]](https://github.com/karunraju/VQA), [[jiasenlu/HieCoAttenVQA]](https://github.com/jiasenlu/HieCoAttenVQA).
- [2016 CVPR] **Stacked Attention Networks for Image Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Stacked%20Attention%20Networks%20for%20Image%20Question%20Answering.bib), sources: [[zcyang/imageqa-san]](https://github.com/zcyang/imageqa-san).
- [2016 CVPR] **Neural Module Networks**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Neural%20Module%20Networks.bib).
- [2016 EMNLP] **Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**, [[paper]](https://www.aclweb.org/anthology/D16-1044.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1044.bib), sources: [[akirafukui/vqa-mcb]](https://github.com/akirafukui/vqa-mcb), [[Cadene/vqa.pytorch]](https://github.com/Cadene/vqa.pytorch), [[MarcBS/keras]](https://github.com/MarcBS/keras).
- [2017 CVPR] **Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering**, [[paper]](https://zpascal.net/cvpr2017/Goyal_Making_the_v_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Making%20the%20V%20in%20VQA%20Matter%20-%20Elevating%20the%20Role%20of%20Image%20Understanding%20in%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/).
- [2018 CVPR] **Visual Grounding via Accumulated Attention**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20via%20Accumulated%20Attention.bib).
- [2018 CVPR] **Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering**, [[paper]](https://zpascal.net/cvpr2018/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Dont%20Just%20Assume%20Look%20and%20Answer%20-%20Overcoming%20Priors%20for%20Visual%20Question%20Answering.bib), [[homepage]](https://www.cc.gatech.edu/~aagrawal307/vqa-cp/), sources: [[AishwaryaAgrawal/GVQA]](https://github.com/AishwaryaAgrawal/GVQA).
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2018 CVPR] **Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Bottom-Up%20and%20Top-Down%20Attention%20for%20Image%20Captioning%20and%20Visual%20Question%20Answering.bib), sources: [[peteanderson80/bottom-up-attention]](https://github.com/peteanderson80/bottom-up-attention), [[hengyuan-hu/bottom-up-attention-vqa]](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [[LeeDoYup/bottom-up-attention-tf]](https://github.com/LeeDoYup/bottom-up-attention-tf).
- [2018 NeurIPS] **Overcoming Language Priors in Visual Question Answering with Adversarial Regularization**, [[paper]](http://papers.nips.cc/paper/7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization.pdf), [[bibtex]](/Bibtex/Overcoming%20Language%20Priors%20in%20Visual%20Question%20Answering%20with%20Adversarial%20Regularization.bib).
- [2019 AAAI] **Dynamic Capsule Attention for Visual Question Answering**, [[paper]](/Documents/Papers/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.pdf), [[bibtex]](/Bibtex/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.bib).
- [2019 AAAI] **BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection**, [[paper]](https://arxiv.org/pdf/1902.00038.pdf), [[bibtex]](/Bibtex/Block.bib), sources: [[Cadene/block.bootstrap.pytorch]](https://github.com/Cadene/block.bootstrap.pytorch).
- [2019 ACL] **Multi-grained Attention with Object-level Grounding for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/P19-1349.pdf), [[bibtex]](/Bibtex/Multi-grained%20Attention%20with%20Object-level%20Grounding%20for%20Visual%20Question%20Answering.bib).
- [2019 ACL] **A Corpus for Reasoning About Natural Language Grounded in Photographs**, [[paper]](https://www.aclweb.org/anthology/P19-1644v2.pdf), [[bibtex]](/Bibtex/A%20Corpus%20for%20Reasoning%20About%20Natural%20Language%20Grounded%20in%20Photographs.bib), [[homepage]](http://lil.nlp.cornell.edu/nlvr/).
- [2019 EMNLP] **B2T2: Fusion of Detected Objects in Text for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/D19-1219.pdf), [[bibtex]](/Bibtex/Fusion%20of%20Detected%20Objects%20in%20Text%20for%20Visual%20Question%20Answering.bib), sources: [[google-research/language/language/question_answering/b2t2/]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2).
- [2019 ICCV] **Multi-modality Latent Interaction Network for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Multi-modality%20Latent%20Interaction%20Network%20for%20Visual%20Question%20Answering.bib).
- [2019 ICCV] **Dynamic Graph Attention for Referring Expression Comprehension**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Dynamic%20Graph%20Attention%20for%20Referring%20Expression%20Comprehension.bib), sources: [[sibeiyang/sgmn]](https://github.com/sibeiyang/sgmn).
- [2019 SIGIR] **Quantifying and Alleviating the Language Prior Problem in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1905.04877.pdf), [[bibtex]](/Bibtex/Quantifying%20and%20Alleviating%20the%20Language%20Prior%20Problem%20in%20Visual%20Question%20Answering.bib), sources: [[guoyang9/vqa-prior]](https://github.com/guoyang9/vqa-prior).
- [2019 CVPR] **Towards VQA Models That Can Read**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Towards%20VQA%20Models%20That%20Can%20Read.bib), sources: [[facebookresearch/pythia]](https://github.com/facebookresearch/pythia).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).
- [2019 CVPR] **Learning to Compose Dynamic Tree Structures for Visual Contexts**, [[paper]](https://zpascal.net/cvpr2019/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Learning%20to%20Compose%20Dynamic%20Tree%20Structures%20for%20Visual%20Contexts.bib), sources: [[KaihuaTang/VCTree-Scene-Graph-Generation]](https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation).
- [2019 CVPR] **Explicit Bias Discovery in Visual Question Answering Models**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Explicit%20Bias%20Discovery%20in%20Visual%20Question%20Answering%20Models.bib).
- [2019 CVPR] **GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/GQA.bib), [[homepage]](visualreasoning.net).
- [2019 NeurIPS] **RUBi: Reducing Unimodal Biases for Visual Question Answering**, [[paper]](http://papers.nips.cc/paper/8371-rubi-reducing-unimodal-biases-for-visual-question-answering.pdf), [[bibtex]](/Bibtex/RUBi%20-%20Reducing%20Unimodal%20Biases%20for%20Visual%20Question%20Answering.bib), sources: [[cdancette/rubi.bootstrap.pytorch]](https://github.com/cdancette/rubi.bootstrap.pytorch).
- [2019 NeurIPS] **TAB-VCR: Tags and Attributes based VCR Baselines**, [[paper]](https://papers.nips.cc/paper/9693-tab-vcr-tags-and-attributes-based-vcr-baselines.pdf), [[bibtex]](/Bibtex/TAB-VCR%20-%20Tags%20and%20Attributes%20based%20VCR%20Baselines.bib), [[slides]](https://deanplayerljx.github.io/tabvcr/neurips_2019_slides.pdf), [[homepage]](https://deanplayerljx.github.io/tabvcr/), sources: [[Deanplayerljx/tab-vcr]](https://github.com/Deanplayerljx/tab-vcr).
- [2020 AAAI] **ManyModalQA: Modality Disambiguation and QA over Diverse Inputs**, [[paper]](https://arxiv.org/pdf/2001.08034.pdf), [[bibtex]](/Bibtex/ManyModalQA.bib), sources: [[hannandarryl/ManyModalQA]](https://github.com/hannandarryl/ManyModalQA).
- [2020 ACL] **Multimodal Neural Graph Memory Networks for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.643.pdf), [[bibtex]](/Bibtex/Multimodal%20Neural%20Graph%20Memory%20Networks%20for%20Visual%20Question%20Answering.bib).
- [2020 ArXiv] **Regularizing Attention Networks for Anomaly Detection in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/2009.10054.pdf), [[bibtex]](/Bibtex/Regularizing%20Attention%20Networks%20for%20Anomaly%20Detection%20in%20Visual%20Question%20Answering.bib), sources: [[LeeDoYup/Anomaly_Detection_VQA]](https://github.com/LeeDoYup/Anomaly_Detection_VQA).
- [2021 WACV] **Meta Module Network for Compositional Visual Reasoning**, [[paper]](https://arxiv.org/pdf/1910.03230.pdf), [[bibtex]](/Bibtex/Meta%20Module%20Network%20for%20Compositional%20Visual%20Reasoning.bib), sources: [[wenhuchen/Meta-Module-Network]](https://github.com/wenhuchen/Meta-Module-Network).

### Image-based Visual Dialogue
- [2019 ArXiv] **Two Causal Principles for Improving Visual Dialog: Visual Dialog Challenge 2019 Winner Report**, [[paper]](https://arxiv.org/pdf/1911.10496.pdf), [[bibtex]](/Bibtex/Two%20Causal%20Principles%20for%20Improving%20Visual%20Dialog%20-%20Visual%20Dialog%20Challenge%202019%20Winner%20Report.bib).

### Multi-task and Meta Learning
- [2020 CVPR] **12-in-1: Multi-Task Vision and Language Representation Learning**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/12-in-1%20-%20Multi-Task%20Vision%20and%20Language%20Representation%20Learning.bib), [[supplementary]](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Lu_12-in-1_Multi-Task_Vision_CVPR_2020_supplemental.pdf).


## Video-based Visual Grounding
### Vision-and-Language Modelling
- [2019 ICCV] **VideoBERT: A Joint Model for Video and Language Representation Learning**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/VideoBERT%20-%20A%20Joint%20Model%20for%20Video%20and%20Language%20Representation%20Learning.bib).
- [2019 ArXiv] **Learning Video Representations Using Contrastive Bidirectional Transformer**, [[paper]](https://arxiv.org/pdf/1906.05743.pdf), [[bibtex]](/Bibtex/Learning%20Video%20Representations%20Using%20Contrastive%20Bidirectional%20Transformer.bib).
- [2020 CVPR] **End-to-End Learning of Visual Representations from Uncurated Instructional Videos**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Miech_End-to-End_Learning_of_Visual_Representations_From_Uncurated_Instructional_Videos_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/End-to-End%20Learning%20of%20Visual%20Representations%20from%20Uncurated%20Instructional%20Videos.bib), [[homepage]](https://www.di.ens.fr/willow/research/mil-nce/), sources: [[antoine77340/MIL-NCE_HowTo100M]](https://github.com/antoine77340/MIL-NCE_HowTo100M), [[MIL-NCE TFHub]](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/text_to_video_retrieval_with_s3d_milnce.ipynb#scrollTo=nwv4ZQ4qmak5).

### Video Captioning
- [2015 ICCV] **Sequence to Sequence: Video to Text**, [[paper]](http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf), [[bibtex]](/Bibtex/Sequence%20to%20Sequence%20–%20Video%20to%20Text.bib), [[homepage]](https://vsubhashini.github.io/s2vt.html), sources: [[vsubhashini/caffe/examples/s2vt]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt).
- [2017 ICCV] **Dense-Captioning Events in Videos**, [[paper]](https://arxiv.org/pdf/1705.00754.pdf), [[bibtex]](/Bibtex/Dense-Captioning%20Events%20in%20Videos.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/densevid/), source: [[ranjaykrishna/densevid_eval]](https://github.com/ranjaykrishna/densevid_eval).
- [2017 ArXiv] **Multi-Task Video Captioning with Video and Entailment Generation**, [[paper]](https://arxiv.org/pdf/1704.07489.pdf), [[bibtex]](/Bibtex/Multi-Task%20Video%20Captioning%20with%20Video%20and%20Entailment%20Generation.bib).
- [2018 CVPR] **Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**, [[paper]](https://arxiv.org/pdf/1804.00100.pdf), [[bibtex]](/Bibtex/Bidirectional%20Attentive%20Fusion%20with%20Context%20Gating%20for%20Dense%20Video%20Captioning.bib), sources: [[JaywongWang/DenseVideoCaptioning]](https://github.com/JaywongWang/DenseVideoCaptioning).
- [2018 CVPR] **End-to-End Dense Video Captioning with Masked Transformer**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/End-to-End%20Dense%20Video%20Captioning%20with%20Masked%20Transformer.bib), sources: [[salesforce/densecap]](https://github.com/salesforce/densecap).
- [2018 CVPR] **Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos**, [[paper]](http://vision.stanford.edu/pdf/huang-buch-2018cvpr), [[bibtex]](/Bibtex/Finding%20It%20-%20Weakly-Supervised%20Reference-Aware%20Visual%20Grounding%20in%20Instructional%20Videos.bib), [[supplementary]](https://finding-it.github.io/finding-it-suppmat.pdf), [[poster]](https://drive.google.com/file/d/1uvnw6VDn0r1nS3ePyFKaCbEx5GZw1ZEy/view), [[homepage]](https://finding-it.github.io), [[youtube]](https://www.youtube.com/watch?v=GBo4sFNzhtU&feature=youtu.be&t=1366).
- [2018 NeurIPS] **Weakly Supervised Dense Event Captioning in Videos**, [[paper]](https://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf), [[bibtex]](/Bibtex/Weakly%20Supervised%20Dense%20Event%20Captioning%20in%20Videos.bib), sources: [[XgDuan/WSDEC]](https://github.com/XgDuan/WSDEC).
- [2019 WACV] **Joint Event Detection and Description in Continuous Video Streams**, [[paper]](http://www.boyangli.co/paper/huijuanxu-wacv-2019.pdf), [[bibtex]](/Bibtex/Joint%20Event%20Detection%20and%20Description%20in%20Continuous%20Video%20Streams.bib), sources: [[VisionLearningGroup/JEDDi-Net]](https://github.com/VisionLearningGroup/JEDDi-Net).
- [2019 CVPR] **Grounded Video Description**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Grounded%20Video%20Description.bib), sources: [[facebookresearch/ActivityNet-Entities]](https://github.com/facebookresearch/ActivityNet-Entities), [[facebookresearch/grounded-video-description]](https://github.com/facebookresearch/grounded-video-description).
- [2019 CSUR] **Video Description: A Survey of Methods, Datasets, and Evaluation Metrics**, [[paper]](https://arxiv.org/pdf/1806.00186.pdf), [[bibtex]](/Bibtex/Video%20Description%20-%20A%20Survey%20of%20Methods%20Datasets%20and%20Evaluation%20Metrics.bib).
- [2019 ACL] **Dense Procedure Captioning in Narrated Instructional Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1641.pdf), [[bibtex]](/Bibtex/Dense%20Procedure%20Captioning%20in%20Narrated%20Instructional%20Videos.bib).
- [2019 ACL] **Multimodal Abstractive Summarization for How2 Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1659.pdf), [[bibtex]](/Bibtex/Multimodal%20Abstractive%20Summarization%20for%20How2%20Videos.bib).
- [2020 ICCV] **VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/VATEX.bib), [[homepage]](http://vatex.org/main/index.html).
- [2020 ACL] **MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.233.pdf), [[bibtex]](/Bibtex/MART.bib), sources: [[jayleicn/recurrent-transformer]](https://github.com/jayleicn/recurrent-transformer).

### Video-level Retrieval
- [2015 AAAI] **Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework**, [[paper]](https://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf), [[bibtex]](/Bibtex/Jointly%20Modeling%20Deep%20Video%20and%20Compositional%20Text%20to%20Bridge%20Vision%20and%20Language%20in%20a%20Unified%20Framework.bib).
- [2018 ECCV] **Find and Focus: Retrieve and Localize Video Events with Natural Language Queries**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Dian_SHAO_Find_and_Focus_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Find%20and%20Focus%20-%20Retrieve%20and%20Localize%20Video%20Events%20with%20Natural%20Language%20Queries.bib).

### Natural Language Video Localization (Temporal Video Grounding)
- [2015 ICCV] **Weakly-Supervised Alignment of Video With Text**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Bojanowski_Weakly-Supervised_Alignment_of_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Alignment%20of%20Video%20With%20Text.bib).
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://arxiv.org/pdf/1708.01641.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2017 ICCV] **TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals**, [[paper]](https://arxiv.org/pdf/1703.06189.pdf), [[supplementary]](http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Gao_TURN_TAP_Temporal_ICCV_2017_supplemental.pdf), [[bibtex]](/Bibtex/TURN%20TAP%20-%20Temporal%20Unit%20Regression%20Network%20for%20Temporal%20Action%20Proposals.bib), sources: [[jiyanggao/TURN-TAP]](https://github.com/jiyanggao/TURN-TAP).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://www.aclweb.org/anthology/D18-1168.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/D18-1168.Attachment.pdf), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 EMNLP] **Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/D18-1015.pdf), [[bibtex]](/Bibtex/Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib).
- [2018 ECCV] **Video Re-localization**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Video%20Re-localization.bib), sources: [[fengyang0317/video_reloc]](https://github.com/fengyang0317/video_reloc).
- [2018 ECCV] **Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bingbin_Liu_Temporal_Modular_Networks_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Temporal%20Modular%20Networks%20for%20Retrieving%20Complex%20Compositional%20Activities%20in%20Videos.bib), [[homepage]](https://clarabing.github.io/tmn/).
- [2018 ECCV] **Find and Focus: Retrieve and Localize Video Events with Natural Language Queries**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Dian_SHAO_Find_and_Focus_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Find%20and%20Focus%20-%20Retrieve%20and%20Localize%20Video%20Events%20with%20Natural%20Language%20Queries.bib).
- [2018 ACMMM] **Cross-modal Moment Localization in Videos**, [[paper]](https://liqiangnie.github.io/paper/p843-liu.pdf), [[bibtex]](/Bibtex/Cross-modal%20Moment%20Localization%20in%20Videos.bib).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).
- [2018 ArXiv] **Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning**, [[paper]](http://cs-people.bu.edu/hxu/arxiv-version-Text-to-Clip.pdf), [[bibtex]](/Bibtex/Text-to-Clip%20Video%20Retrieval%20with%20Early%20Fusion%20and%20Re-Captioning.bib).
- [2018 SIGIR] **Attentive Moment Retrieval in Videos**, [[paper]](http://staff.ustc.edu.cn/~hexn/papers/sigir18-video-retrieval.pdf), [[bibtex]](/Bibtex/Attentive%20Moment%20Retrieval%20in%20Videos.bib), [[slides]](https://pdfs.semanticscholar.org/5dc8/f69ad9404ed9e8d2318dca19f4eb534440a5.pdf), [[codes]](https://sigir2018.wixsite.com/acrn).
- [2019 WACV] **MAC: Mining Activity Concepts for Language-based Temporal Localization**, [[paper]](https://arxiv.org/pdf/1811.08925.pdf), [[bibtex]](/Bibtex/MAC%20-%20Mining%20Activity%20Concepts%20for%20Language-based%20Temporal%20Localization.bib), sources: [[runzhouge/MAC]](https://github.com/runzhouge/MAC).
- [2019 NAACL] **ExCL: Extractive Clip Localization Using Natural Language Descriptions**, [[paper]](https://www.aclweb.org/anthology/N19-1198.pdf), [[bibtex]](/Bibtex/ExCL%20-%20Extractive%20Clip%20Localization%20Using%20Natural%20Language%20Descriptions.bib).
- [2019 CVPR] **MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/MAN%20-%20Moment%20Alignment%20Network%20for%20Natural%20Language%20Moment%20Retrieval%20via%20Iterative%20Graph%20Adjustment.bib).
- [2019 CVPR] **Language-driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Language-driven%20Temporal%20Activity%20Localization%20-%20A%20Semantic%20Matching%20Reinforcement%20Learning%20Model.bib).
- [2019 CVPR] **Weakly Supervised Video Moment Retrieval From Text Queries**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Mithun_Weakly_Supervised_Video_Moment_Retrieval_From_Text_Queries_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Weakly%20Supervised%20Video%20Moment%20Retrieval%20From%20Text%20Queries.bib), sources: [[niluthpol/weak_supervised_video_moment]](https://github.com/niluthpol/weak_supervised_video_moment).
- [2019 AAAI] **Localizing Natural Language in Videos**, [[paper]](http://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf), [[bibtex]](/Bibtex/Localizing%20Natural%20Language%20in%20Videos.bib).
- [2019 AAAI] **Multilevel Language and Vision Integration for Text-to-Clip Retrieval**, [[paper]](https://arxiv.org/pdf/1804.05113.pdf), [[bibtex]](/Bibtex/Multilevel%20Language%20and%20Vision%20Integration%20for%20Text-to-Clip%20Retrieval.bib), sources: [[VisionLearningGroup/Text-to-Clip_Retrieval]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval).
- [2019 AAAI] **To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression**, [[paper]](https://arxiv.org/pdf/1804.07014.pdf), [[bibtex]](/Bibtex/To%20Find%20Where%20You%20Talk%20-%20Temporal%20Sentence%20Localization%20in%20Video%20with%20Attention%20Based%20Location%20Regression.bib).
- [2019 AAAI] **Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos**, [[paper]](https://arxiv.org/pdf/1901.06829.pdf), [[bibtex]](/Bibtex/Read%20Watch%20and%20Move%20-%20Reinforcement%20Learning%20for%20Temporally%20Grounding%20Natural%20Language%20Descriptions%20in%20Videos.bib).
- [2019 AAAI] **Semantic Proposal for Activity Localization in Videos via Sentence Query**, [[paper]](https://pdfs.semanticscholar.org/8548/d5a93869a5a4c808f5e81742f59f848c718c.pdf?_ga=2.88458585.398432507.1574674952-963912669.1574674952), [[bibtex]](/Bibtex/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.bib).
- [2019 ACMMM] **Exploiting Temporal Relationships in Video Moment Localization with Natural Language**, [[paper]](https://arxiv.org/pdf/1908.03846.pdf), [[bibtex]](/Bibtex/Exploiting%20Temporal%20Relationships%20in%20Video%20Moment%20Localization%20with%20Natural%20Language.bib), sources: [[Sy-Zhang/TCMN-Release]](https://github.com/Sy-Zhang/TCMN-Release).
- [2019 SIGIR] **Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos**, [[paper]](https://arxiv.org/pdf/1906.02497.pdf), [[bibtex]](/Bibtex/Cross-Modal%20Interaction%20Networks%20for%20Query-Based%20Moment%20Retrieval%20in%20Videos.bib), sources: [[ikuinen/CMIN_moment_retrieval]](https://github.com/ikuinen/CMIN_moment_retrieval).
- [2019 ICCV] **Temporal Localization of Moments in Video Collections with Natural Language**, [[paper]](https://arxiv.org/pdf/1907.12763.pdf), [[bibtex]](/Bibtex/Temporal%20Localization%20of%20Moments%20in%20Video%20Collections%20with%20Natural%20Language.bib), sources: [[escorciav/moments-retrieval-page]](https://github.com/escorciav/moments-retrieval-page).
- [2019 EMNLP] **DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization**, [[paper]](https://www.aclweb.org/anthology/D19-1518.pdf), [[bibtex]](/Bibtex/DEBUG%20-%20A%20Dense%20Bottom-Up%20Grounding%20Approach%20for%20Natural%20Language%20Video%20Localization.bib).
- [2019 NeurIPS] **Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos**, [[paper]](https://papers.nips.cc/paper/8344-semantic-conditioned-dynamic-modulation-for-temporal-sentence-grounding-in-videos.pdf), [[bibtex]](/Bibtex/Semantic%20Conditioned%20Dynamic%20Modulation%20for%20Temporal%20Sentence%20Grounding%20in%20Videos.bib), sources: [[yytzsy/SCDM]](https://github.com/yytzsy/SCDM).
- [2019 ArXiv] **LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval**, [[paper]](https://arxiv.org/pdf/1909.13784.pdf), [[bibtex]](/Bibtex/LoGAN.bib).
- [2020 BMVC] **Tripping through time: Efficient Localization of Activities in Videos**, [[paper]](https://arxiv.org/pdf/1904.09936.pdf), [[bibtex]](/Bibtex/Tripping%20through%20time%20-%20Efficient%20Localization%20of%20Activities%20in%20Videos.bib).
- [2020 WACV] **Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention**, [[paper]](http://openaccess.thecvf.com/content_WACV_2020/papers/Rodriguez_Proposal-free_Temporal_Moment_Localization_of_a_Natural-Language_Query_in_Video_WACV_2020_paper.pdf), [[bibtex]](/Bibtex/Proposal-free%20Temporal%20Moment%20Localization%20of%20a%20Natural-Language%20Query%20in%20Video%20using%20Guided%20Attention.bib), sources: [[crodriguezo/TMLGA]](https://github.com/crodriguezo/TMLGA).
- [2020 AAAI] **Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, [[paper]](https://arxiv.org/pdf/1912.03590.pdf), [[bibtex]](/Bibtex/Learning%202D%20Temporal%20Adjacent%20Networks%20for%20Moment%20Localization%20with%20Natural%20Language.bib), sources: [[microsoft/2D-TAN]](https://github.com/microsoft/2D-TAN).
- [2020 AAAI] **Weakly-Supervised Video Moment Retrieval via Semantic Completion Network**, [[paper]](https://arxiv.org/pdf/1911.08199.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Video%20Moment%20Retrieval%20via%20Semantic%20Completion%20Network.bib).
- [2020 AAAI] **Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video**, [[paper]](https://arxiv.org/pdf/2001.06680.pdf), [[bibtex]](/Bibtex/Tree-Structured%20Policy%20based%20Progressive%20Reinforcement%20Learning%20for%20Temporally%20Language%20Grounding%20in%20Video.bib), sources: [[WuJie1010/TSP-PRL]](https://github.com/WuJie1010/TSP-PRL).
- [2020 AAAI] **Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction**, [[paper]](https://arxiv.org/pdf/1909.05010.pdf), [[bibtex]](/Bibtex/Temporally%20Grounding%20Language%20Queries%20in%20Videos%20by%20Contextual%20Boundary-aware%20Prediction.bib), sources: [[JaywongWang/CBP]](https://github.com/JaywongWang/CBP).
- [2020 AAAI] **Rethinking the Bottom-Up Framework for Query-based Video Localization**, [[paper]](https://zjuchenlong.github.io/papers/AAAI_2020.pdf), [[bibtex]](/Bibtex/Rethinking%20the%20Bottom-Up%20Framework%20for%20Query-based%20Video%20Localization.bib).
- [2020 ACL] **Span-based Localizing Network for Natural Language Video Localization**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.585.pdf), [[bibtex]](https://www.aclweb.org/anthology/2020.acl-main.585.bib), sources: [[IsaacChanghau/VSLNet]](https://github.com/IsaacChanghau/VSLNet).
- [2020 CVPR] **Dense Regression Network for Video Grounding**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Dense_Regression_Network_for_Video_Grounding_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Dense%20Regression%20Network%20for%20Video%20Grounding.bib), [[supplementary]](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zeng_Dense_Regression_Network_CVPR_2020_supplemental.pdf), sources: [[Alvin-Zeng/DRN]](https://github.com/Alvin-Zeng/DRN).
- [2020 CVPR] **Local-Global Video-Text Interactions for Temporal Grounding**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Mun_Local-Global_Video-Text_Interactions_for_Temporal_Grounding_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Local-Global%20Video-Text%20Interactions%20for%20Temporal%20Grounding.bib), sources: [[JonghwanMun/LGI4temporalgrounding]](https://github.com/JonghwanMun/LGI4temporalgrounding).
- [2020 ECCV] **Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos**, [[paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490324.pdf), [[bibtex]](/Bibtex/Learning%20Modality%20Interaction%20for%20Temporal%20Sentence%20Localization%20and%20Event%20Captioning%20in%20Videos.bib).

### Natural Language Video Localization (Spatio-Temporal Video Grounding)
- [2019 ACL] **Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/P19-1183.pdf), [[bibtex]](/Bibtex/Weakly-Supervised%20Spatio-Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/P19-1183.Supplementary.pdf), sources: [[JeffCHEN2017/WSSTG]](https://github.com/JeffCHEN2017/WSSTG).
- [2020 CVPR] **Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Where_Does_It_Exist_Spatio-Temporal_Video_Grounding_for_Multi-Form_Sentences_CVPR_2020_paper.pdf), [[bibtex]](/Bibtex/Where%20Does%20It%20Exist.bib), sources: [[Guaranteer/VidSTG-Dataset]](https://github.com/Guaranteer/VidSTG-Dataset).


### Video Question Answering & Reasoning
- [2017 CVPR] **TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering**, [[paper]](http://zpascal.net/cvpr2017/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/TGIF-QA%20-%20Toward%20Spatio-Temporal%20Reasoning%20in%20Visual%20Question%20Answering.bib), sources: [[YunseokJANG/tgif-qa]](https://github.com/YunseokJANG/tgif-qa).
- [2018 CVPR] **Motion-Appearance Co-Memory Networks for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Motion-Appearance%20Co-Memory%20Networks%20for%20Video%20Question%20Answering.bib).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2018 EMNLP] **TVQA: Localized Compositional Video Question Answering**, [[paper]](https://www.aclweb.org/anthology/D18-1167.pdf), [[bibtex]](/Bibtex/TVQA%20-%20Localized%20Compositional%20Video%20Question%20Answering.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/D18-1167.Attachment.pdf), [[homepage]](http://tvqa.cs.unc.edu), sources: [[jayleicn/TVQA]](https://github.com/jayleicn/TVQA).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 CVPR] **Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Heterogeneous%20Memory%20Enhanced%20Multimodal%20Attention%20Model%20for%20Video%20Question%20Answering.bib), [[poster]](http://homes.sice.indiana.edu/fan6/docs/cvpr19_videoqa.pdf), sources: [[fanchenyou/HME-VideoQA]](https://github.com/fanchenyou/HME-VideoQA).
- [2019 AAAI] **ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering**, [[paper]](https://arxiv.org/pdf/1906.02467.pdf), [[bibtex]](/Bibtex/ActivityNet-QA%20-%20A%20Dataset%20for%20Understanding%20Complex%20Web%20Videos%20via%20Question%20Answering.bib), sources: [[MILVLG/activitynet-qa]](https://github.com/MILVLG/activitynet-qa).
- [2019 ArXiv] **TVQA+: Spatio-Temporal Grounding for Video Question Answering**, [[paper]](https://arxiv.org/pdf/1904.11574.pdf), [[bibtex]](/Bibtex/TVQA+%20-%20Spatio-Temporal%20Grounding%20for%20Video%20Question%20Answering.bib), [[homepage]](http://tvqa.cs.unc.edu).
- [2020 AAAI] **Divide and Conquer: Question-Guided Spatio-Temporal Contextual Attention for Video Question Answering**, [[paper]](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-JiangJ.1655.pdf), [[bibtex]](/Bibtex/Divide%20and%20Conquer%20-%20Question-Guided%20Spatio-Temporal%20Contextual%20Attention%20for%20Video%20Question%20Answering.bib).
- [2020 ICLR] **CLEVRER: Collision Events for Video Representation and Reasoning**, [[paper]](https://openreview.net/pdf?id=HkxYzANYDB), [[bibtex]](/Bibtex/CLEVRER%20-%20Collision%20Events%20for%20Video%20Representation%20and%20Reasoning.bib), [[homepage]](http://clevrer.csail.mit.edu/).

### Video Grounded Dialogue
- [2019 ACL] **Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems**, [[paper]](https://www.aclweb.org/anthology/P19-1564.pdf), [[bibtex]](/Bibtex/Multimodal%20Transformer%20Networks%20for%20End-to-End%20Video-Grounded%20Dialogue%20Systems.bib), sources: [[henryhungle/MTN]](https://github.com/henryhungle/MTN).


## Vision-and-Language Navigation
- [2018 CVPR] **Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Vision-and-Language%20Navigation.bib), sources: [[peteanderson80/Matterport3DSimulator]](https://github.com/peteanderson80/Matterport3DSimulator).
- [2019 ACL] **Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation**, [[paper]](https://www.aclweb.org/anthology/P19-1655.pdf), [[bibtex]](/Bibtex/Are%20You%20Looking%20Grounding%20to%20Multiple%20Modalities%20in%20Vision-and-Language%20Navigation.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/P19-1655.Supplementary.pdf).


## Others
- [2019 IJCAI] **Adapting BERT for Target-Oriented Multimodal Sentiment Classification**, [[paper]](https://www.ijcai.org/proceedings/2019/0751.pdf), [[bibtex]](/Bibtex/Adapting%20BERT%20for%20Target-Oriented%20Multimodal%20Sentiment%20Classification.bib), sources: [[jefferyYu/TomBERT]](https://github.com/jefferyYu/TomBERT).
- [2019 CVPR] **Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Generalized%20Intersection%20over%20Union%20-%20A%20Metric%20and%20A%20Loss%20for%20Bounding%20Box%20Regression.bib), [[homepage]](https://giou.stanford.edu), sources: [[generalized-iou]](https://github.com/generalized-iou).
- [2020 CVPR] **Visual Grounding in Video for Unsupervised Word Translation**, [[paper]](https://arxiv.org/pdf/2003.05078.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20in%20Video%20for%20Unsupervised%20Word%20Translation.bib), sources: [[gsig/visual-grounding]](https://github.com/gsig/visual-grounding).